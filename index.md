---
date: 2015-11-11T17:18:58+03:00
author: Samet Demir
layout: page
---

<style>
.profile-container {
  display:flex; flex-wrap:wrap; align-items:flex-start; justify-content:center;
  gap:2rem; margin:2rem auto;
}
.profile-image-section { flex-shrink:0; text-align:center; }
.profile-content { flex:1; min-width:300px; text-align:justify; }
@media (max-width:768px){ .profile-container{flex-direction:column; align-items:center;} .profile-content{text-align:center;} }
.section-title, h4.section-title { margin-top:1.5rem; font-weight:700; }
.small { font-size:.95rem; }
ul.compact{ margin:.5rem 0 0 1.2rem; } ul.compact li{ margin:.2rem 0; }
.badge{ display:inline-block; padding:.15rem .5rem; border-radius:999px; font-size:.8rem; background:#f2f2f2; margin-right:.4rem; }
.link-row a{ margin-right:.8rem; }
</style>

<div class="profile-container">
  <div class="profile-image-section">
    <img src="/uploads/photo.jpg" alt="Photo of Samet Demir" width="197" height="236" />
    <p><em><strong>E-mail: </strong><a href="mailto:sdemir20@ku.edu.tr">sdemir20@ku.edu.tr</a></em></p>
    <p><em><strong>Address: </strong><a href="https://maps.app.goo.gl/bcJnWHBz3V6pZhxm6">ENG147, Koc University <br/><span style="float:right;">İstanbul / Turkey</span></a></em></p>
  </div>

  <div class="profile-content">
    <p>
      I’m a Ph.D. candidate in <a href="https://cs.ku.edu.tr/">Computer Science &amp; Engineering</a> at <a href="https://www.ku.edu.tr/en/">Koc University</a> and an AI Fellow at the <a href="https://ai.ku.edu.tr/">KUIS AI Research Center</a>. I am a senior member of the Machine Learning &amp; Information Processing (MLIP) group, advised by <a href="https://mysite.ku.edu.tr/zdogan/">Prof. Zafer Dogan</a>.
    </p>
    <p>
      <em>My research develops a high-dimensional learning theory for how the statistics of learned representations arise from the interplay of architecture, data, and optimization—and how these statistics govern behaviors like generalization, in-context learning, and robustness in modern deep learning.</em>
    </p>

    <h4 class="section-title">Research interests</h4>
    <p class="small">
      <span class="badge">Learning theory</span>
      <span class="badge">High-dimensional statistics</span>
      <span class="badge">Deep learning</span>
      <span class="badge">Gaussian universality</span>
      <span class="badge">Random matrix theory</span>
      <span class="badge">Robust learning</span>
    </p>

    <h4 class="section-title">Selected publications</h4>
    <ul class="compact small">
      <li><strong>Demir</strong> &amp; Dogan. <a href="https://openreview.net/forum?id=615vk8hmeH"><em>How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs.</em></a> <strong>NeurIPS 2025</strong>.</li>
      <li><strong>Demir</strong> &amp; Dogan. <a href="https://openreview.net/forum?id=tNn6Hskmti"><em>Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures with Structure.</em></a> <strong>ICLR 2025</strong>.</li>
      <li><strong>Demir</strong> &amp; Dogan. <a href="https://arxiv.org/abs/2511.01292"><em>Optimal Attention Temperature Enhances ICL under Distribution Shift.</em></a> arXiv 2025.</li>
      <li><strong>Demir</strong> &amp; Dogan. <a href="https://arxiv.org/abs/2409.20250"><em>Random features outperform linear models: Effect of strong input-label correlation in spiked covariance data.</em></a> arXiv 2024.</li>
      <li>More on <a href="https://scholar.google.com/citations?user=VKyOvOYAAAAJ">Google Scholar</a>.</li>
    </ul>

    <h4 class="section-title">Academic service</h4>
    <ul class="compact small">
      <li><b>Reviewer</b>: ICLR (2025–26), NeurIPS 2025, ACML (2024–25).</li>
    </ul>

    <h4 class="section-title">Teaching</h4>
    <ul class="compact small">
      <li><b>Head TA</b>: Introduction to Modern Learning Theory (2025– ); Programming for Engineers (2024– ).</li>
      <li><b>TA</b>: Algorithms &amp; Complexity (2023–24), Software Engineering (2023), Programming with Python (2022), Computer Systems &amp; Programming (2020–22), Introduction to Computing (2017).</li>
    </ul>
  </div>
</div>
